# Build Request: conversation-context

## Overview
Add conversation context injection so model switches and wake messages have access to recent chat history.

**Problem:**
- When Haiku generates a wake message, it has no context if Rob replies to it
- When we switch models mid-conversation (Haiku → Sonnet, etc.), the new model loses the thread
- This causes awkward exchanges and breaks conversational flow

**Solution:**
Implement a rolling conversation buffer that captures the last 10-20 message exchanges and injects them as context for every response.

**Implementation Details:**

1. **Conversation Buffer (src/discord.js)**
   - Maintain a per-channel Map of recent messages
   - Structure: `conversationBuffers.set(channelId, [...messages])`
   - Each entry: `{ role: 'user'|'assistant', content: string, timestamp: ISO string }`
   - Keep last 20 messages (10 exchanges)
   - Auto-clear buffer after 30 minutes of inactivity

2. **Context Injection (src/claude.js)**
   - Accept new parameter: `conversationContext` (array of recent messages)
   - If provided and non-empty, inject before the user's current message as a system block:
     ```
     --- Recent Conversation Context ---
     [formatted message history]
     --- End Context ---
     ```
   - Apply to both regular message handling AND wake message generation

3. **Buffer Management**
   - Add user messages to buffer before sending to Claude
   - Add assistant responses to buffer after receiving from Claude
   - Implement cleanup: if last message timestamp > 30min ago, clear buffer
   - Keep buffer lightweight (plain text, no tool calls or complex formatting)

4. **Wake Message Integration**
   - Pass conversation buffer to wake message generator
   - If Rob replies to a wake message, Haiku will see the wake message + reply in context

5. **Model Switching Integration**
   - Conversation buffer persists across model switches
   - When switching from Haiku → Sonnet, Sonnet gets the full buffer including what Haiku said

**File Changes:**
- `src/discord.js` — Add buffer management, pass to claude.js
- `src/wake.js` — Accept and use conversationContext for wake messages
- `src/claude.js` — Accept and inject conversationContext parameter

**Testing:**
- Test wake message reply (Haiku should remember context)
- Test model switch mid-conversation (Sonnet should see Haiku's messages)
- Test buffer cleanup after idle period
- Test multiple channels don't cross-contaminate

## Skill Architecture
This is a MiniClaw Discord bot skill. Each skill is a self-contained folder with:
- `handler.js` — The main module. MUST export:
  - `toolDefinition` — An Anthropic tool_use schema object ({ name, description, input_schema })
  - `execute(input)` — An async function that receives the tool input and returns a result object
- `SKILL.md` — Documentation: what the skill does, when to use it, example trigger phrases
- `PROGRESS.md` — Development log (append-only, timestamped entries)
- `data/` — Optional persistent data directory for JSON/text files

## Technical Requirements
- ES modules (import/export, no require())
- Node.js 20+ (no browser APIs)
- The skill runs inside a Discord bot on Windows — no network calls unless explicitly needed
- `process.cwd()` returns the MiniClaw project root
- The handler is loaded via dynamic import: `import('file:///path/to/handler.js')`
- All file operations should use `path.resolve()` for Windows compatibility
- Return objects from execute() — they get JSON-serialized and sent back to Claude API as tool results
- Errors should be returned as `{ success: false, error: 'message' }`, not thrown (unless truly fatal)

## Tool Definition Pattern
```javascript
export const toolDefinition = {
  name: 'conversation_context',
  description: 'Clear description of what this tool does and when to use it',
  input_schema: {
    type: 'object',
    properties: {
      action: {
        type: 'string',
        enum: ['action1', 'action2'],
        description: 'What operation to perform'
      }
      // ... more properties
    },
    required: ['action']
  }
};

export async function execute(input) {
  const { action } = input;
  switch (action) {
    case 'action1':
      return { success: true, data: '...' };
    default:
      return { success: false, error: `Unknown action: ${action}` };
  }
}
```

## Specific Requirements
- Must work with existing model switching logic
- Must not break current wake message system
- Keep token overhead reasonable (20 messages max)
- Buffer must be per-channel (support DMs and multiple channels)
- Auto-cleanup after 30 minutes idle
- Should work seamlessly with Haiku/Sonnet/Opus switches
- This is a CORE MODIFICATION, not a skill — changes go directly to src/ files
## Example Trigger Phrases
These are the kinds of things the user might say that should activate this tool:
- "respond to wake message"
- "switch models mid-chat"
- "remember what I just said"
- "context across model switches"
## Data Storage Notes
In-memory conversation buffer only (no persistence needed)

## What to Build
1. Create `handler.js` with the full working implementation
2. Create `SKILL.md` with clear documentation
3. Create `PROGRESS.md` with an initial entry
4. Create the `data/` directory if the skill needs persistent storage
5. Make sure the code actually works — no placeholder TODOs

## Important
- The tool name in toolDefinition.name MUST be: `conversation_context`
- The skill folder is: `skills/conversation-context/`
- Write COMPLETE, WORKING code. Not stubs. Not placeholders. Real, functional code.
- Test your logic mentally — walk through the execute() function with sample inputs.


## Additional Instructions (2026-02-15T05:01:23.193Z)
IMPORTANT: Change the rolling conversation buffer size from 20 messages to 5 messages. Rob wants to keep token costs down — 20 is too excessive. Use 5 as the buffer size throughout the implementation.


## Additional Instructions (2026-02-15T05:24:09.727Z)
## Additional Requirement: Seed Buffer from Daily Logs on Startup

The conversation buffer should be pre-populated on bot startup by reading the most recent daily log. This ensures the wake-up message has context even though the in-memory buffer starts empty.

**Implementation:**
1. On bot startup (in `src/discord.js` after client is ready), read today's daily log file
2. Parse the log to extract the last 5 message exchanges (user + assistant pairs)
3. Populate the conversation buffer for the wake channel with these messages
4. Format them the same way as runtime buffer entries: `{ role, content, timestamp }`
5. This should happen BEFORE the wake message is generated

**Edge cases:**
- If today's log doesn't exist or is empty, just start with an empty buffer (normal cold start)
- If the log has fewer than 5 messages, use whatever is available
- Don't crash if log parsing fails — gracefully fall back to empty buffer

This way the wake message will have conversational continuity across restarts.


## Additional Instructions (2026-02-15T05:40:26.779Z)
## CRITICAL FIX: Log Assistant Responses Too

The daily log currently only captures user messages, but NOT assistant responses. This creates an incomplete conversation record and breaks the buffer seeding logic (it will only see one side of the conversation).

**Required change to `src/discord.js`:**

After receiving the assistant's response from Claude and sending it to Discord, **also log it to the daily log** using the same `memory_append_daily` pattern:

```javascript
// After sending the response to Discord:
await memory_append_daily(`User: ${message.content}`); // Already there
// ... get response from Claude ...
await message.channel.send(reply);

// ADD THIS:
await memory_append_daily(`Assistant: ${reply}`);
```

This ensures:
1. Complete conversation transcripts in daily logs
2. Buffer seeding on startup works correctly (gets both sides)
3. Future memory searches have full context
4. Conversation history is accurate for debugging/review

Make sure to handle multi-part responses (if reply is split due to Discord length limits, log the full unsplit version).
